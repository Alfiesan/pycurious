<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.2" />
<title>pycurious.parallel API documentation</title>
<meta name="description" content="Copyright 2018 Ben Mather, Robert Delhaye â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pycurious.parallel</code></h1>
</header>
<section id="section-intro">
<p>Copyright 2018 Ben Mather, Robert Delhaye</p>
<p>This file is part of PyCurious.</p>
<p>PyCurious is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or any later version.</p>
<p>PyCurious is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
See the
GNU Lesser General Public License for more details.</p>
<p>You should have received a copy of the GNU Lesser General Public License
along with PyCurious.
If not, see <a href="http://www.gnu.org/licenses/">http://www.gnu.org/licenses/</a>.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;
Copyright 2018 Ben Mather, Robert Delhaye

This file is part of PyCurious.

PyCurious is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or any later version.

PyCurious is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.

You should have received a copy of the GNU Lesser General Public License
along with PyCurious.  If not, see &lt;http://www.gnu.org/licenses/&gt;.
&#34;&#34;&#34;

# -*- coding: utf-8 -*-
import numpy as np
import warnings
from .optimise import CurieOptimise

try: range=xrange
except: pass


class CurieParallel(CurieOptimise):
    &#34;&#34;&#34;
    A parallel implementation built on the PETSc DMDA mesh structure
    Almost a trivially parallel problem: each processor works locally
    on their portion of the grid. A stipulation being that the window
    size must not exceed the thickness of shadow zones or else the
    subgrid will be truncated.

    This inherits from CurieOptimise and CurieGrid classes.


    Parameters
    ----------
     grid       : 2D array of magnetic data
     xmin       : minimum x bound in metres
     xmax       : maximum x bound in metres
     ymin       : minimum y bound in metres
     ymax       : maximum y bound in metres
     max_window : maximum size of the windows
                : this will be enforced for all methods
    
    Attributes
    ----------
     bounds     : lower and upper bounds for beta, zt, dz, C
     prior      : dictionary of priors for beta, zt, dz, C
     coords     : local xy coordinates
    &#34;&#34;&#34;

    def __init__(self, grid, xmin, xmax, ymin, ymax, max_window):

        from petsc4py import PETSc
        from mpi4py import MPI
        comm = MPI.COMM_WORLD
        self.comm = comm
        self.MPI = MPI

        # super(CurieParallel, self).__init__(grid, xmin, xmax, ymin, ymax)

        # determine stencil width (should be window size/2)
        ny, nx = grid.shape
        dx = (xmax - xmin)/nx
        dy = (ymax - ymin)/ny
        nw = max_window/dx
        n2w = int(nw/2) + 1 # add some buffer

        if abs(dx - dy) &gt; 1.0:
            warnings.warn(&#34;node spacing should be identical {}&#34;.format((dx,dy)), RuntimeWarning)


        dm = PETSc.DMDA().create(dim=2, sizes=(nx,ny), stencil_width=n2w, comm=comm)
        dm.setUniformCoordinates(xmin, xmax, ymin, ymax)

        self.dm = dm
        self.lgmap = dm.getLGMap()
        self.lvec = dm.createLocalVector()
        self.gvec = dm.createGlobalVector()

        coords = dm.getCoordinatesLocal().array.reshape(-1, 2)
        xmin, ymin = coords.min(axis=0)
        xmax, ymax = coords.max(axis=0)
        self.coords = coords
        self.max_window = max_window

        (imin, imax), (jmin, jmax) = dm.getGhostRanges()

        # now decompose grid for each processor

        grid_chunk = grid[jmin:jmax, imin:imax]

        super(CurieParallel, self).__init__(grid_chunk, xmin, xmax, ymin, ymax)

        reduce_methods = {&#39;sum&#39;: MPI.SUM, &#39;max&#39;: MPI.MAX, &#39;min&#39;: MPI.MIN, &#39;mean&#39;: MPI.SUM}
        self._reduce_methods = reduce_methods


    def subgrid(self, xc, yc, window):
        &#34;&#34;&#34;
        Extract a subgrid from the data at a window around
        the point (xc,yc)
        
        Parameters
        ----------
         xc      : x coordinate
         yc      : y coordinate
         window  : size of window in metres

        Returns
        -------
         data    : subgrid
        &#34;&#34;&#34;
        if window &gt; self.max_window:
            raise ValueError(&#34;Max window size is {}&#34;.format(self.max_window))

        return super(CurieParallel, self).subgrid(window, xc, yc)

    
    def sync(self, vector):
        &#34;&#34;&#34;
        Synchronise a local vector across all processors
        &#34;&#34;&#34;
        self.lvec.setArray(vector)
        self.dm.localToGlobal(self.lvec, self.gvec)
        self.dm.globalToLocal(self.gvec, self.lvec)
        return self.lvec.array.copy()


    def add_parallel_prior(self, **kwargs):
        &#34;&#34;&#34;
        Add a prior to the dictionary (tuple)
        This version broadcasts just the values from the root processor

        Available priors are beta, zt, dz, C

        Usage
        -----
         add_parallel_prior(beta=(p, sigma_p))
        &#34;&#34;&#34;

        comm = self.comm
        MPI = self.MPI

        for key in kwargs:
            if key in self.prior:
                prior = kwargs[key]
                p  = np.array(prior[0]) # prior
                dp = np.array(prior[1]) # uncertainty

                comm.Bcast([p, MPI.DOUBLE], root=0)
                comm.Bcast([dp,MPI.DOUBLE], root=0)

                self.prior[key] = (float(p), float(dp))
            else:
                raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))


    def distribute_prior(self, method, **kwargs):
        &#34;&#34;&#34;
        Distribute priors across all processors using a specific method.

        Parameters
        ----------
         method  : operation to reduce the local priors to a single value
                 : choose from one of &#39;sum&#39;, &#39;mean&#39;, &#39;min&#39;, &#39;max&#39;
         kwargs  : (prior, sigma_prior) tuple

        Notes
        -----
         add_prior will broadcast the prior on the root processor (rank=0)
         distribute_prior enacts a MPI Allreduce operation.
        &#34;&#34;&#34;

        comm = self.comm
        MPI = self.MPI

        if method in self._reduce_methods:
            op = self._reduce_methods[method]
        else:
            raise ValueError(&#34;choose one of the following methods {}&#34;.format(self._reduce_methods.keys()))

        for key in kwargs:
            if key in self.prior:
                prior = kwargs[key]
                local_p  = np.array(prior[0]) # prior
                local_dp = np.array(prior[1]) # uncertainty

                global_p = np.array(0.0)
                global_dp = np.array(0.0)

                comm.Allreduce([local_p, MPI.DOUBLE], [global_p, MPI.DOUBLE], op=op)
                comm.Allreduce([local_dp, MPI.DOUBLE], [global_dp, MPI.DOUBLE], op=op)
                if method == &#39;mean&#39;:
                    global_p  /= comm.size
                    global_dp /= comm.size

                self.prior[key] = (float(global_p), float(global_dp))
            else:
                raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pycurious.parallel.CurieParallel"><code class="flex name class">
<span>class <span class="ident">CurieParallel</span></span>
<span>(</span><span>grid, xmin, xmax, ymin, ymax, max_window)</span>
</code></dt>
<dd>
<section class="desc"><p>A parallel implementation built on the PETSc DMDA mesh structure
Almost a trivially parallel problem: each processor works locally
on their portion of the grid. A stipulation being that the window
size must not exceed the thickness of shadow zones or else the
subgrid will be truncated.</p>
<p>This inherits from CurieOptimise and CurieGrid classes.</p>
<h2 id="parameters">Parameters</h2>
<p>grid
: 2D array of magnetic data
xmin
: minimum x bound in metres
xmax
: maximum x bound in metres
ymin
: minimum y bound in metres
ymax
: maximum y bound in metres
max_window : maximum size of the windows
: this will be enforced for all methods</p>
<h2 id="attributes">Attributes</h2>
<p>bounds
: lower and upper bounds for beta, zt, dz, C
prior
: dictionary of priors for beta, zt, dz, C
coords
: local xy coordinates</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CurieParallel(CurieOptimise):
    &#34;&#34;&#34;
    A parallel implementation built on the PETSc DMDA mesh structure
    Almost a trivially parallel problem: each processor works locally
    on their portion of the grid. A stipulation being that the window
    size must not exceed the thickness of shadow zones or else the
    subgrid will be truncated.

    This inherits from CurieOptimise and CurieGrid classes.


    Parameters
    ----------
     grid       : 2D array of magnetic data
     xmin       : minimum x bound in metres
     xmax       : maximum x bound in metres
     ymin       : minimum y bound in metres
     ymax       : maximum y bound in metres
     max_window : maximum size of the windows
                : this will be enforced for all methods
    
    Attributes
    ----------
     bounds     : lower and upper bounds for beta, zt, dz, C
     prior      : dictionary of priors for beta, zt, dz, C
     coords     : local xy coordinates
    &#34;&#34;&#34;

    def __init__(self, grid, xmin, xmax, ymin, ymax, max_window):

        from petsc4py import PETSc
        from mpi4py import MPI
        comm = MPI.COMM_WORLD
        self.comm = comm
        self.MPI = MPI

        # super(CurieParallel, self).__init__(grid, xmin, xmax, ymin, ymax)

        # determine stencil width (should be window size/2)
        ny, nx = grid.shape
        dx = (xmax - xmin)/nx
        dy = (ymax - ymin)/ny
        nw = max_window/dx
        n2w = int(nw/2) + 1 # add some buffer

        if abs(dx - dy) &gt; 1.0:
            warnings.warn(&#34;node spacing should be identical {}&#34;.format((dx,dy)), RuntimeWarning)


        dm = PETSc.DMDA().create(dim=2, sizes=(nx,ny), stencil_width=n2w, comm=comm)
        dm.setUniformCoordinates(xmin, xmax, ymin, ymax)

        self.dm = dm
        self.lgmap = dm.getLGMap()
        self.lvec = dm.createLocalVector()
        self.gvec = dm.createGlobalVector()

        coords = dm.getCoordinatesLocal().array.reshape(-1, 2)
        xmin, ymin = coords.min(axis=0)
        xmax, ymax = coords.max(axis=0)
        self.coords = coords
        self.max_window = max_window

        (imin, imax), (jmin, jmax) = dm.getGhostRanges()

        # now decompose grid for each processor

        grid_chunk = grid[jmin:jmax, imin:imax]

        super(CurieParallel, self).__init__(grid_chunk, xmin, xmax, ymin, ymax)

        reduce_methods = {&#39;sum&#39;: MPI.SUM, &#39;max&#39;: MPI.MAX, &#39;min&#39;: MPI.MIN, &#39;mean&#39;: MPI.SUM}
        self._reduce_methods = reduce_methods


    def subgrid(self, xc, yc, window):
        &#34;&#34;&#34;
        Extract a subgrid from the data at a window around
        the point (xc,yc)
        
        Parameters
        ----------
         xc      : x coordinate
         yc      : y coordinate
         window  : size of window in metres

        Returns
        -------
         data    : subgrid
        &#34;&#34;&#34;
        if window &gt; self.max_window:
            raise ValueError(&#34;Max window size is {}&#34;.format(self.max_window))

        return super(CurieParallel, self).subgrid(window, xc, yc)

    
    def sync(self, vector):
        &#34;&#34;&#34;
        Synchronise a local vector across all processors
        &#34;&#34;&#34;
        self.lvec.setArray(vector)
        self.dm.localToGlobal(self.lvec, self.gvec)
        self.dm.globalToLocal(self.gvec, self.lvec)
        return self.lvec.array.copy()


    def add_parallel_prior(self, **kwargs):
        &#34;&#34;&#34;
        Add a prior to the dictionary (tuple)
        This version broadcasts just the values from the root processor

        Available priors are beta, zt, dz, C

        Usage
        -----
         add_parallel_prior(beta=(p, sigma_p))
        &#34;&#34;&#34;

        comm = self.comm
        MPI = self.MPI

        for key in kwargs:
            if key in self.prior:
                prior = kwargs[key]
                p  = np.array(prior[0]) # prior
                dp = np.array(prior[1]) # uncertainty

                comm.Bcast([p, MPI.DOUBLE], root=0)
                comm.Bcast([dp,MPI.DOUBLE], root=0)

                self.prior[key] = (float(p), float(dp))
            else:
                raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))


    def distribute_prior(self, method, **kwargs):
        &#34;&#34;&#34;
        Distribute priors across all processors using a specific method.

        Parameters
        ----------
         method  : operation to reduce the local priors to a single value
                 : choose from one of &#39;sum&#39;, &#39;mean&#39;, &#39;min&#39;, &#39;max&#39;
         kwargs  : (prior, sigma_prior) tuple

        Notes
        -----
         add_prior will broadcast the prior on the root processor (rank=0)
         distribute_prior enacts a MPI Allreduce operation.
        &#34;&#34;&#34;

        comm = self.comm
        MPI = self.MPI

        if method in self._reduce_methods:
            op = self._reduce_methods[method]
        else:
            raise ValueError(&#34;choose one of the following methods {}&#34;.format(self._reduce_methods.keys()))

        for key in kwargs:
            if key in self.prior:
                prior = kwargs[key]
                local_p  = np.array(prior[0]) # prior
                local_dp = np.array(prior[1]) # uncertainty

                global_p = np.array(0.0)
                global_dp = np.array(0.0)

                comm.Allreduce([local_p, MPI.DOUBLE], [global_p, MPI.DOUBLE], op=op)
                comm.Allreduce([local_dp, MPI.DOUBLE], [global_dp, MPI.DOUBLE], op=op)
                if method == &#39;mean&#39;:
                    global_p  /= comm.size
                    global_dp /= comm.size

                self.prior[key] = (float(global_p), float(global_dp))
            else:
                raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pycurious.optimise.CurieOptimise" href="optimise.html#pycurious.optimise.CurieOptimise">CurieOptimise</a></li>
<li><a title="pycurious.grid.CurieGrid" href="grid.html#pycurious.grid.CurieGrid">CurieGrid</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pycurious.parallel.CurieParallel.add_parallel_prior"><code class="name flex">
<span>def <span class="ident">add_parallel_prior</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Add a prior to the dictionary (tuple)
This version broadcasts just the values from the root processor</p>
<p>Available priors are beta, zt, dz, C</p>
<h2 id="usage">Usage</h2>
<p>add_parallel_prior(beta=(p, sigma_p))</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_parallel_prior(self, **kwargs):
    &#34;&#34;&#34;
    Add a prior to the dictionary (tuple)
    This version broadcasts just the values from the root processor

    Available priors are beta, zt, dz, C

    Usage
    -----
     add_parallel_prior(beta=(p, sigma_p))
    &#34;&#34;&#34;

    comm = self.comm
    MPI = self.MPI

    for key in kwargs:
        if key in self.prior:
            prior = kwargs[key]
            p  = np.array(prior[0]) # prior
            dp = np.array(prior[1]) # uncertainty

            comm.Bcast([p, MPI.DOUBLE], root=0)
            comm.Bcast([dp,MPI.DOUBLE], root=0)

            self.prior[key] = (float(p), float(dp))
        else:
            raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))</code></pre>
</details>
</dd>
<dt id="pycurious.parallel.CurieParallel.distribute_prior"><code class="name flex">
<span>def <span class="ident">distribute_prior</span></span>(<span>self, method, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Distribute priors across all processors using a specific method.</p>
<h2 id="parameters">Parameters</h2>
<p>method
: operation to reduce the local priors to a single value
: choose from one of 'sum', 'mean', 'min', 'max'
kwargs
: (prior, sigma_prior) tuple</p>
<h2 id="notes">Notes</h2>
<p>add_prior will broadcast the prior on the root processor (rank=0)
distribute_prior enacts a MPI Allreduce operation.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def distribute_prior(self, method, **kwargs):
    &#34;&#34;&#34;
    Distribute priors across all processors using a specific method.

    Parameters
    ----------
     method  : operation to reduce the local priors to a single value
             : choose from one of &#39;sum&#39;, &#39;mean&#39;, &#39;min&#39;, &#39;max&#39;
     kwargs  : (prior, sigma_prior) tuple

    Notes
    -----
     add_prior will broadcast the prior on the root processor (rank=0)
     distribute_prior enacts a MPI Allreduce operation.
    &#34;&#34;&#34;

    comm = self.comm
    MPI = self.MPI

    if method in self._reduce_methods:
        op = self._reduce_methods[method]
    else:
        raise ValueError(&#34;choose one of the following methods {}&#34;.format(self._reduce_methods.keys()))

    for key in kwargs:
        if key in self.prior:
            prior = kwargs[key]
            local_p  = np.array(prior[0]) # prior
            local_dp = np.array(prior[1]) # uncertainty

            global_p = np.array(0.0)
            global_dp = np.array(0.0)

            comm.Allreduce([local_p, MPI.DOUBLE], [global_p, MPI.DOUBLE], op=op)
            comm.Allreduce([local_dp, MPI.DOUBLE], [global_dp, MPI.DOUBLE], op=op)
            if method == &#39;mean&#39;:
                global_p  /= comm.size
                global_dp /= comm.size

            self.prior[key] = (float(global_p), float(global_dp))
        else:
            raise ValueError(&#34;prior must be one of {}&#34;.format(self.prior.keys()))</code></pre>
</details>
</dd>
<dt id="pycurious.parallel.CurieParallel.subgrid"><code class="name flex">
<span>def <span class="ident">subgrid</span></span>(<span>self, xc, yc, window)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a subgrid from the data at a window around
the point (xc,yc)</p>
<h2 id="parameters">Parameters</h2>
<p>xc
: x coordinate
yc
: y coordinate
window
: size of window in metres</p>
<h2 id="returns">Returns</h2>
<p>data
: subgrid</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subgrid(self, xc, yc, window):
    &#34;&#34;&#34;
    Extract a subgrid from the data at a window around
    the point (xc,yc)
    
    Parameters
    ----------
     xc      : x coordinate
     yc      : y coordinate
     window  : size of window in metres

    Returns
    -------
     data    : subgrid
    &#34;&#34;&#34;
    if window &gt; self.max_window:
        raise ValueError(&#34;Max window size is {}&#34;.format(self.max_window))

    return super(CurieParallel, self).subgrid(window, xc, yc)</code></pre>
</details>
</dd>
<dt id="pycurious.parallel.CurieParallel.sync"><code class="name flex">
<span>def <span class="ident">sync</span></span>(<span>self, vector)</span>
</code></dt>
<dd>
<section class="desc"><p>Synchronise a local vector across all processors</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sync(self, vector):
    &#34;&#34;&#34;
    Synchronise a local vector across all processors
    &#34;&#34;&#34;
    self.lvec.setArray(vector)
    self.dm.localToGlobal(self.lvec, self.gvec)
    self.dm.globalToLocal(self.gvec, self.lvec)
    return self.lvec.array.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pycurious.optimise.CurieOptimise" href="optimise.html#pycurious.optimise.CurieOptimise">CurieOptimise</a></b></code>:
<ul class="hlist">
<li><code><a title="pycurious.optimise.CurieOptimise.add_prior" href="optimise.html#pycurious.optimise.CurieOptimise.add_prior">add_prior</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.azimuthal_spectrum" href="grid.html#pycurious.grid.CurieGrid.azimuthal_spectrum">azimuthal_spectrum</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.create_centroid_list" href="grid.html#pycurious.grid.CurieGrid.create_centroid_list">create_centroid_list</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.metropolis_hastings" href="optimise.html#pycurious.optimise.CurieOptimise.metropolis_hastings">metropolis_hastings</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.min_func" href="optimise.html#pycurious.optimise.CurieOptimise.min_func">min_func</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.objective_function" href="optimise.html#pycurious.optimise.CurieOptimise.objective_function">objective_function</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.objective_routine" href="optimise.html#pycurious.optimise.CurieOptimise.objective_routine">objective_routine</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.optimise" href="optimise.html#pycurious.optimise.CurieOptimise.optimise">optimise</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.optimise_routine" href="optimise.html#pycurious.optimise.CurieOptimise.optimise_routine">optimise_routine</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.parallelise_routine" href="optimise.html#pycurious.optimise.CurieOptimise.parallelise_routine">parallelise_routine</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.radial_spectrum" href="grid.html#pycurious.grid.CurieGrid.radial_spectrum">radial_spectrum</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.radial_spectrum_log" href="grid.html#pycurious.grid.CurieGrid.radial_spectrum_log">radial_spectrum_log</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.reduce_to_pole" href="grid.html#pycurious.grid.CurieGrid.reduce_to_pole">reduce_to_pole</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.remove_trend_linear" href="grid.html#pycurious.grid.CurieGrid.remove_trend_linear">remove_trend_linear</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.reset_priors" href="optimise.html#pycurious.optimise.CurieOptimise.reset_priors">reset_priors</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.sensitivity" href="optimise.html#pycurious.optimise.CurieOptimise.sensitivity">sensitivity</a></code></li>
<li><code><a title="pycurious.optimise.CurieOptimise.upward_continuation" href="grid.html#pycurious.grid.CurieGrid.upward_continuation">upward_continuation</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pycurious" href="index.html">pycurious</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pycurious.parallel.CurieParallel" href="#pycurious.parallel.CurieParallel">CurieParallel</a></code></h4>
<ul class="">
<li><code><a title="pycurious.parallel.CurieParallel.add_parallel_prior" href="#pycurious.parallel.CurieParallel.add_parallel_prior">add_parallel_prior</a></code></li>
<li><code><a title="pycurious.parallel.CurieParallel.distribute_prior" href="#pycurious.parallel.CurieParallel.distribute_prior">distribute_prior</a></code></li>
<li><code><a title="pycurious.parallel.CurieParallel.subgrid" href="#pycurious.parallel.CurieParallel.subgrid">subgrid</a></code></li>
<li><code><a title="pycurious.parallel.CurieParallel.sync" href="#pycurious.parallel.CurieParallel.sync">sync</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>